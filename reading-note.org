* 读书笔记
** 领导和管理
   + 领导是与“拉”相关的活动，设定目的地和通往目的地的路线图
   + 管理是与“推”相关的活动，设法到达目的地







* 周边小常识
** unicode和UTF-8
    unicode是一个编码规范，它纳入了世界上所有的字符，赋予每个字符一个唯一编码。如一个汉字在unicode里对应一个2字节编码，这个编码在支持unicode的系统里都能被正确识别。
    单从编码规范的角度来看，unicode跟计算机（可以）没有任何关系。
    unicode定义了字符的编码（码值），但是并未定义计算机中该如何存储、传送内容，所以，出现了UTF(unicode transformation format)，有UTF-8和UTF-16。UTF-16即任何字符都用两个字节存放，这对英文为主的环境显然太浪费，但是对中文为主的环境比较适合。UTF-8是变长编码（1到3字节），解决了浪费问题，缺点是编码规则复杂一些。
    
*** utf-8的表示形式
    utf8根据当前字节的第一个bit来确定使用几个字节来表示（存储）一个unicode，这是约定的规则.你用UTF-8来表示时必须遵守这样的规则.我们知道UTF-16不需要用啥字符来做标志,所以两字节也就是2的16次能表示65536个字符.而UTF-8由于里面有额外的标志信息,所有一个字节只能表示2的7次方128个字符,两个字节只能表示2的11次方2048个字符.而三个字节能表示2的16次方,65536个字符.由于"汉"的编码27721大于2048了所有两个字节还不够,只能用三个字节来表示.

    0xxxxxxx,如果是这样的01串,也就是以0开头后面是啥就不用管了XX代表任意bit.就表示把一个字节做为一个单元.就跟ASCII完全一样.
    110xxxxx 10xxxxxx.如果是这样的格式,则把两个字节当一个单元
    1110xxxx 10xxxxxx 10xxxxxx 如果是这种格式则是三个字节当一个单元.
    
    所有要用1110xxxx 10xxxxxx 10xxxxxx这种格式.把27721对应的二进制从左到右填充XXX符号，于是就出现了Big-Endian,Little-Endian的术语.Big-Endian就是从左到右,Little-Endian是从右到左.
    由上面我们可以看出UTF-8需要判断每个字节中的开头标志信息,所以如果一当某个字节在传送过程中出错了,就会导致后面的字节也会解析出错.而UTF-16不会判断开头标志,即使错也只会错一个字符,所以容错能力强.

*** 如何区分文件是哪种编码
    前面说了要知道具体是哪种编码方式,需要判断文本开头的标志,下面是所有编码对应的开头标志

    EF BB BF　　　 UTF-8
    FE FF　　　　　UTF-16/UCS-2, little endian
    FF FE　　　　　UTF-16/UCS-2, big endian
    FF FE 00 00　　UTF-32/UCS-4, little endian.
    00 00 FE FF　　UTF-32/UCS-4, big-endian.

    其中的UCS就是前面说的ISO制定的标准,和Unicode是完全一样的,只不过名字不一样.ucs-2对应utf-16,ucs-4对应UTF-32.UTF-8是没有对应的UCS


* python 学习
** 类与继承
   1. Type函数可以动态生成类型（class），实质上pyton创建类的过程也是调用type()函数
   2. type()函数可以动态创建类，metaclass可以动态控制类（增加方法、属性和过程），类似java动态生成类（动态字节码）
   3. 装饰器在python中类似动态代理，装饰一个函数的调用，但是不影响函数执行。
   4. 枚举类需继承Enum，直接通过ClassName.enumName获取枚举

** 生成器generator
   与列表生成式的样子很像，只不过generator使用小括号包裹，而列表生成式用中括号，
   如:g = (x * x for x in range(10))
   
   generator只定义了计算规则，一边循环一遍计算。反复调用next()方法获取下一个返回值。一般永远不会使用next方法，因为generator用for循环来迭代它。
   如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator。yield关键字在generator中负责中断，下次调用next()在此中断的地方开始。


* java 并发编程
** 重排序
   java程序从编译到运行，会经历3个重排序过程，分别是
   1. 编译重排序
   2. 指令重排序
   3. 内存系统重排序


* java 基础
** java 集合数据结构
   1. HashMap内部数据结构是一个二元结构，外层是一个数组，数组内元素则是一个列表。存取删元素时，会先通过key的hashcode取模，确定key在数组中的位置，再从链表中通过equal确定真正的key。
      + 取模运算速度不及位运算，在hashMap中，巧妙的使用位运算来达到取模的效果，h & (length -1)。这决定了hashMap的length必须为2的次方
      + 在存取删过程中，全程都是key在参与运算，valve做为附属信息只在最后存入了数据中
        
   2. TreeMap是一个有序的Map，其内部使用红黑树保证排序。
      + 红黑树有几个特性，1 根节点和叶子节点必须为黑色；2 红色的子节点必须为黑色；3 任意节点到到该节点的子孙节点所有路径上包含相同数目的黑节点。红黑树是接近平衡的二叉树
      + 红黑树的常用操作为左旋和右旋

* DB 基础
** B树 B-树 B+树 B*树
   1. B树既是二叉搜索树，有如下特性。在接近平衡二叉树时，效率最高（相当于二分查找），但是在进过多次增删后，可能会出现所有元素朝向一边（线性）的情况。所以如何保证B树结构分布均匀的平衡算法是平衡二叉树的关键。平衡算法是B树中插入和删除节点时的策略
      + 所有非叶子节点至多拥有两个儿子（left和right）
      + 非叶子节点的左指针指向小于其关键字的子树，右指针指向大于其关键字的子树
      + 所有节点存储一个关键字
        
   2. B-树是多路搜索树
      + 同时限定了层级和分叉个数
      + 关键字也是分布在整棵树中
      + 非根非叶子节点里的关键字表明了子树的数值范围，同时也表明了子树的个数。如关键字为8、12，那么会有3棵子树分别小于8、大于8小于12、大于12
      + 搜索可以在非叶子节点中结束
      + 元素有M/2的限定，在插入结点时，如果结点已满，需要将结点分裂为两个各占M/2的结点；删除结点时，需将两个不足M/2的兄弟结点合并；


   1. B+树跟B-树类似，也是多路搜索树
      + 非根非叶子节点不存储关键字，所有关键字均在叶子节点，非根非叶子节点即作为叶子节点的索引（适合存储和文件系统）
      + 非根非叶子节点也是存储子树的数值范围，但是范围略有不同，如关键字8和12，那么会有2棵子树，分别是大于8小于12、大约12
      + 叶子节点持有数据链表指针，用于搜索数据

   2. B* 树
      + 为非根非叶子节点也增加一个链表指针
      + 将节点的利用率从1/2提高到2/3

* 高等数学
** 主要内容
一元、多元函数的微分学和积分学，矢量代数，空间解析几何，无穷级数和微分方程

** 学习目的
掌握高等数学的基本知识，基本理论，基本计算方法，提高数学素养。
培养抽象思维和逻辑推理的能力，和辩证的思想方法。
培养空间想象能力，分析问题和解决问题的能力
* luence
** 如何创建索引，步骤如下
*** 第一步，原文档，document，可以是文本，文档（word）、html；
*** 第二步，将原文档传递给分词组件（Tokenizer）
分词组件会做以下几件事情，此过程称为tokenize
1. 将文档分为一个一个独立的单词；
2. 去除标点符号；
3. 去掉停词；
*** 第三步，将得到词元传给语言处理组件（linguistic Process）
语言处理组件主要对词元做一些同语言相关的处理，例如对英语来说
1. 变为小写；
2. 将单词缩减为词根形式，如cars到car等，这种操作称为stemming；
3. 将单词转变为词根形式，如drove到drive等，这种操作成为lemmatization
*** 第四步，将得到的词（term）传给索引组件（Indexer）
1. 利用得到的词（term）创建一个字典；
2. 对字典按字母顺序排序；
3. 合并相同的词成为文档倒排（Posting List）链表；其中包含document Frequency,Frequency

** 如何查询索引
*** 第一步，查询语句
查询语句同我们普通的语言一样，也有一定语法；
*** 第二步，对查询语句进行词法分析、语法分析及语言处理
1. 词法分析主要用来识别单词和关键字；
2. 语法分析主要根据查询语句的语法规则来形成一颗语法树；
3. 语言处理同创建过程中的语言处理几乎相同；
*** 第四步，根据得到文档和查询语句的相关性，对结果进行排序
1. 找出词（term）对文档的重要性称为计算词的权重（term weight）的过程。计算词的权重有两个参数，一个是词（term），一个是文档（document）
+ term frequency(tf)，即此term在文档中出现了多少次，tf越大说明越重要；
+ document frequency（df），即有多少文档包含此term，df越大越不重要；
+ 根据上面两个参数使用负责的数学公式得出权重
2. 判断词之间的关系从而得到文档相关性的过程应用一种叫向量空间模型的算法（Vector Space model）
+ 我们把文档（document）所有词term的权重（term weight）看作一个向量；
+ 同样把查询语句看作一个简单的文档，也用向量表示；
+ 把所有搜索出的文档向量放到一个N维空间中，每个词是1维；
+ 与查询语句向量夹角越小，则相关性越大。
** luence 文件结构
* elasticsearch
** es 元素
1. 节点，即运行实例
2. index，es中的关键实体，类似与DB中的库。一个es可以拥有多个Index。
3. 分片(shards)，es可以把每个Index分成多个小索引，每个小索引就是分片。
+ 主分片（Primary shard） 索引的子集，索引可以切分成多个分片（默认是5个），分布到不同的集群节点上。分片对应的是 Lucene 中的索引。
+ 副本分片（Replica shard）每个主分片可以有一个或者多个副本（默认1个）
4. type相当于数据库中的table概念，mapping是针对 Type 的。同一个索引里可以包含多个 Type。
5. Mapping 相当于数据库中的schema，用来约束字段的类型，不过 Elasticsearch 的 mapping 可以自动根据数据创建。
6. 文档（Document) 相当于数据库中的row
7. 字段（Field）相当于数据库中的column。
8. 分配（Allocation） 将分片分配给某个节点的过程，包括分配主分片或者副本。如果是副本，还包含从主分片复制数据的过程。
* RocketMQ
** msg producer
1. 实例化DefaultMQProducer类，设置producerGroup,namesrv,instanceName;
2. start(),启动netty
3. send()，获取topic路由信息，queue list，选择broker发送消息

** msg consumer
1. 实例化DefaultMQConsumer类，设置consumrGroup,namesrv,instanceName,topic
2. 注册回调，启动start()
   + 获取topic路由信息，queue list
   + 启动PullService，借助Blockingqueue实现长轮询，快速响应和拉取消息；
   + RebalanceService触发一个CountDownLatch条件，作为PullService的启动点，之后每30s从namesrv拉取路由和queue list，如果发现新路由则加入进来。
